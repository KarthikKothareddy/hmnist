{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>dx</th>\n",
       "      <th>dx_type</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>localization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10015</td>\n",
       "      <td>10015</td>\n",
       "      <td>10015</td>\n",
       "      <td>10015</td>\n",
       "      <td>9958.000000</td>\n",
       "      <td>10015</td>\n",
       "      <td>10015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>7470</td>\n",
       "      <td>10015</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>HAM_0000835</td>\n",
       "      <td>ISIC_0031556</td>\n",
       "      <td>nv</td>\n",
       "      <td>histo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6705</td>\n",
       "      <td>5340</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5406</td>\n",
       "      <td>2192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.863828</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.968614</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          lesion_id      image_id     dx dx_type          age    sex  \\\n",
       "count         10015         10015  10015   10015  9958.000000  10015   \n",
       "unique         7470         10015      7       4          NaN      3   \n",
       "top     HAM_0000835  ISIC_0031556     nv   histo          NaN   male   \n",
       "freq              6             1   6705    5340          NaN   5406   \n",
       "mean            NaN           NaN    NaN     NaN    51.863828    NaN   \n",
       "std             NaN           NaN    NaN     NaN    16.968614    NaN   \n",
       "min             NaN           NaN    NaN     NaN     0.000000    NaN   \n",
       "25%             NaN           NaN    NaN     NaN    40.000000    NaN   \n",
       "50%             NaN           NaN    NaN     NaN    50.000000    NaN   \n",
       "75%             NaN           NaN    NaN     NaN    65.000000    NaN   \n",
       "max             NaN           NaN    NaN     NaN    85.000000    NaN   \n",
       "\n",
       "       localization  \n",
       "count         10015  \n",
       "unique           15  \n",
       "top            back  \n",
       "freq           2192  \n",
       "mean            NaN  \n",
       "std             NaN  \n",
       "min             NaN  \n",
       "25%             NaN  \n",
       "50%             NaN  \n",
       "75%             NaN  \n",
       "max             NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = pd.read_csv(\"../HAM10000_metadata.csv\")\n",
    "metadata.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     lesion_id      image_id   dx dx_type   age   sex localization\n",
      "0  HAM_0000118  ISIC_0027419  bkl   histo  80.0  male        scalp\n",
      "1  HAM_0000118  ISIC_0025030  bkl   histo  80.0  male        scalp\n",
      "2  HAM_0002730  ISIC_0026769  bkl   histo  80.0  male        scalp\n",
      "3  HAM_0002730  ISIC_0025661  bkl   histo  80.0  male        scalp\n",
      "4  HAM_0001466  ISIC_0031633  bkl   histo  75.0  male          ear\n"
     ]
    }
   ],
   "source": [
    "print(metadata.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bkl', 'nv', 'df', 'mel', 'vasc', 'bcc', 'akiec']\n"
     ]
    }
   ],
   "source": [
    "labels = list(metadata.dx.unique())\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nv       6705\n",
      "mel      1113\n",
      "bkl      1099\n",
      "bcc       514\n",
      "akiec     327\n",
      "vasc      142\n",
      "df        115\n",
      "Name: dx, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ffaac08c3c8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax1 = plt.subplots(1, 1, figsize = (10, 5))\n",
    "counts = metadata[\"dx\"].value_counts()\n",
    "print(counts)\n",
    "counts.plot(kind='bar', ax=ax1)\n",
    "#metadata[\"dx\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2000\n",
    "\n",
    "# parse out all classes to individual dataframes\n",
    "nv = metadata[metadata[\"dx\"] == \"nv\"]\n",
    "nv = nv.sample(n=threshold)\n",
    "nv.name = \"nv\"\n",
    "mel = metadata[metadata[\"dx\"] == \"mel\"]\n",
    "mel.name = \"mel\"\n",
    "bkl = metadata[metadata[\"dx\"] == \"bkl\"]\n",
    "bkl.name = \"bkl\"\n",
    "bcc = metadata[metadata[\"dx\"] == \"bcc\"]\n",
    "bcc.name = \"bcc\"\n",
    "akiec = metadata[metadata[\"dx\"] == \"akiec\"]\n",
    "akiec.name = \"akiec\"\n",
    "vasc = metadata[metadata[\"dx\"] == \"vasc\"]\n",
    "vasc.name = \"vasc\"\n",
    "df = metadata[metadata[\"dx\"] == \"df\"]\n",
    "df.name = \"df\"\n",
    "# list out\n",
    "classes = [nv, mel, bkl, bcc, akiec, vasc, df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess accordingly\n",
    "def preprocess(_image):\n",
    "    #_image = cv2.cvtColor(_image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    #_image = cv2.equalizeHist(_image) \n",
    "    #_image = cv2.GaussianBlur(_image, (3,3), 1)\n",
    "    _image = cv2.cvtColor(_image, cv2.COLOR_RGB2HSV)\n",
    "    H,S,V = cv2.split(_image)\n",
    "    _V = cv2.equalizeHist(V) \n",
    "    _image = cv2.merge([H, S, _V])\n",
    "    _image = cv2.cvtColor(_image, cv2.COLOR_HSV2RGB)\n",
    "    \n",
    "    return _image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate into Respective Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 1600, Test: 400 images found for class nv\n",
      "Moving complete for class nv...\n",
      "Training: 890, Test: 223 images found for class mel\n",
      "Moving complete for class mel...\n",
      "Training: 879, Test: 220 images found for class bkl\n",
      "Moving complete for class bkl...\n",
      "Training: 411, Test: 103 images found for class bcc\n",
      "Moving complete for class bcc...\n",
      "Training: 261, Test: 66 images found for class akiec\n",
      "Moving complete for class akiec...\n",
      "Training: 113, Test: 29 images found for class vasc\n",
      "Moving complete for class vasc...\n",
      "Training: 92, Test: 23 images found for class df\n",
      "Moving complete for class df...\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#threshold = 2000\n",
    "# seperate images to each directory \n",
    "os.mkdir(\"ham_labled\")\n",
    "os.mkdir(\"ham_labled/train\")\n",
    "os.mkdir(\"ham_labled/test\")\n",
    "\n",
    "# each class\n",
    "for _class in classes:\n",
    "    _train, _test = train_test_split(_class, test_size = 0.2)\n",
    "    print(\"Training: {}, Test: {} images found for class {}\".format(len(_train), len(_test), _class.name))\n",
    "    # train \n",
    "    os.mkdir(\"ham_labled/train/{}\".format(_class.name))\n",
    "    for i, row in _train.iterrows():\n",
    "        image_id = row[\"image_id\"]\n",
    "        img = cv2.imread(\"../ham10000/{}.jpg\".format(image_id))\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        cv2.imwrite(\"../ham10000/{}.jpg\".format(image_id), img)\n",
    "        shutil.move(\n",
    "            src=\"../ham10000/{}.jpg\".format(image_id), \n",
    "            dst=\"ham_labled/train/{}/\".format(_class.name)\n",
    "        )\n",
    "    # test\n",
    "    os.mkdir(\"ham_labled/test/{}\".format(_class.name))\n",
    "    for i, row in _test.iterrows():\n",
    "        image_id = row[\"image_id\"]\n",
    "        img = cv2.imread(\"../ham10000/{}.jpg\".format(image_id))\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        cv2.imwrite(\"../ham10000/{}.jpg\".format(image_id), img)\n",
    "        shutil.move(\n",
    "            src=\"../ham10000/{}.jpg\".format(image_id), \n",
    "            dst=\"ham_labled/test/{}/\".format(_class.name)\n",
    "        )\n",
    "    print(\"Moving complete for class {}...\".format(_class.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Augumentation\n",
    "\n",
    "Before we can start passing above images through a CNN feature extractor, we will augument images to address class imbalance problem and set a threshold of 5000 images per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "_generator = ImageDataGenerator(\n",
    "    width_shift_range=0.001,\n",
    "    height_shift_range=0.001,\n",
    "    zoom_range=0.01,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    rotation_range=0.99\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "def load_images(directory):\n",
    "    images = [cv2.cvtColor(cv2.imread(os.path.join(directory, file)), cv2.COLOR_BGR2RGB)\n",
    "              for file in os.listdir(directory)]\n",
    "    images = [img_to_array(image, data_format=\"channels_last\") for image in images]\n",
    "    images = np.array(images)\n",
    "    images = images.astype(\"float\")\n",
    "    return images\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augumenting images for class mel...\n",
      "Augumenting images for class bkl...\n",
      "Augumenting images for class bcc...\n",
      "Augumenting images for class akiec...\n",
      "Augumenting images for class vasc...\n",
      "Augumenting images for class df...\n"
     ]
    }
   ],
   "source": [
    "batch = 32\n",
    "for _class in classes:\n",
    "    _name = _class.name\n",
    "    if len(_class) >= threshold:\n",
    "        continue\n",
    "    else:\n",
    "        # what a pain ....PHEW!\n",
    "        if os.path.exists(\"ham_labled/train/{}/.ipynb_checkpoints/\".format(_name)):\n",
    "            shutil.rmtree(\"ham_labled/train/{}/.ipynb_checkpoints/\".format(_name))\n",
    "        print(\"Augumenting images for class {}...\".format(_name))\n",
    "        X = load_images(\"ham_labled/train/{}/\".format(_name))\n",
    "        _auguments = _generator.flow(\n",
    "            x=X,\n",
    "            y=None,\n",
    "            batch_size=batch,\n",
    "            save_format=\"jpg\",\n",
    "            save_prefix=\"augumented\",\n",
    "            save_to_dir=\"ham_labled/train/{}/\".format(_name)\n",
    "        )\n",
    "        for _ in range(int(np.ceil((threshold - X.shape[0])/batch))):\n",
    "            x = next(_auguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the newly generated Images and process them accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imutils\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/0c/659c2bdae8e8ca5ef810b9da02db28feaa29ea448ff36b65a1664ff28142/imutils-0.5.2.tar.gz\n",
      "Building wheels for collected packages: imutils\n",
      "  Running setup.py bdist_wheel for imutils ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/b2/40/59/139d450e68847ef2f27d876d527b13389dac23df0f66526b5d\n",
      "Successfully built imutils\n",
      "\u001b[31mmenpo 0.8.1 has requirement matplotlib<2.0,>=1.4, but you'll have matplotlib 3.0.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mmenpo 0.8.1 has requirement pillow<5.0,>=3.0, but you'll have pillow 5.4.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mmenpo 0.8.1 has requirement scipy<1.0,>=0.16, but you'll have scipy 1.2.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: imutils\n",
      "Successfully installed imutils-0.5.2\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.2.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "13190 total images found...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from imutils import paths\n",
    "except ModuleNotFoundError:\n",
    "    !pip install imutils\n",
    "    from imutils import paths\n",
    "import random\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "image_paths = list(paths.list_images(\"ham_labled/train\"))\n",
    "random.shuffle(image_paths)\n",
    "\n",
    "labels = [cls.split(os.path.sep)[-2] for cls in image_paths]\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)\n",
    "\n",
    "print(\"{} total images found...\".format(len(image_paths)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import callbacks\n",
    "from keras.models import Sequential, Model\n",
    "from keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import ReduceLROnPlateau, LearningRateScheduler\n",
    "from keras.applications import ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNet(object):\n",
    "\n",
    "    def __init__(self, height, width, channels, classes, parameter_scaling):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.channels = channels\n",
    "        self.output_classes = classes\n",
    "        self.scale = parameter_scaling\n",
    "        \n",
    "   \n",
    "    def model(self):\n",
    "        _model = ResNet50(\n",
    "            weights=\"imagenet\",\n",
    "            include_top=False,\n",
    "            input_shape=(self.height, self.width, self.channels)\n",
    "        )\n",
    "        x = _model.output\n",
    "        x = Dropout(0.75)(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(512, activation=\"relu\")(x)\n",
    "        x = Dropout(0.75)(x)\n",
    "        x = Dense(self.output_classes, activation=\"softmax\")(x)\n",
    "        model = Model(inputs=_model.input, outputs=x)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "manager = multiprocessing.Manager()\n",
    "queue = manager.Queue(maxsize=10)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-3\n",
    "EPOCHS = 35\n",
    "PARAMETER_SCALING = 16\n",
    "IM_WIDTH = 224\n",
    "IM_HEIGHT = 224\n",
    "\n",
    "def lr_scheduler(epoch, lr):\n",
    "    adjusted_lr = lr\n",
    "    # 5 to 10\n",
    "    if 5 <= epoch < 10:\n",
    "        adjusted_lr = 1e-4\n",
    "        print(\"Current Learning Rate: {}, Adjusted Learning Rate: {}\".format(\n",
    "            str(lr), str(adjusted_lr)\n",
    "        ))\n",
    "    # > 10\n",
    "    elif epoch >= 10:\n",
    "        adjusted_lr = 1e-5\n",
    "        print(\"Current Learning Rate: {}, Adjusted Learning Rate: {}\".format(\n",
    "            str(lr), str(adjusted_lr)\n",
    "        ))\n",
    "    return adjusted_lr\n",
    "\n",
    "def top_2_accuracy(y_true, y_pred):\n",
    "    return top_k_categorical_accuracy(y_true, y_pred, k=2)\n",
    "\n",
    "def top_3_accuracy(y_true, y_pred):\n",
    "    return top_k_categorical_accuracy(y_true, y_pred, k=3)\n",
    "\n",
    "def train():\n",
    "    \n",
    "        h_callbacks = [\n",
    "            callbacks.TensorBoard(\n",
    "                log_dir=\"tensorboard\",\n",
    "                write_graph=True,\n",
    "                write_images=False\n",
    "            ),\n",
    "            LearningRateScheduler(lr_scheduler)\n",
    "        ]\n",
    "        _model = CustomNet(\n",
    "            height=IM_HEIGHT,\n",
    "            width=IM_WIDTH,\n",
    "            channels=3,\n",
    "            classes=7,\n",
    "            parameter_scaling=PARAMETER_SCALING\n",
    "        ).model()\n",
    "        _model.compile(\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            optimizer=Adam(lr=LR),\n",
    "            metrics=[\"accuracy\", categorical_accuracy, top_2_accuracy, top_3_accuracy]\n",
    "        )\n",
    "        \n",
    "        # generators made according to the post\n",
    "        # https://github.com/keras-team/keras/issues/5862\n",
    "        data_generator = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            validation_split=0.2\n",
    "        )\n",
    "        train_generator = data_generator.flow_from_directory(\n",
    "            directory=\"ham_labled/train\",\n",
    "            target_size=(IM_WIDTH, IM_HEIGHT),\n",
    "            shuffle=True,\n",
    "            color_mode=\"rgb\",\n",
    "            class_mode=\"categorical\",\n",
    "            batch_size=BATCH_SIZE,\n",
    "            subset=\"training\"\n",
    "        )\n",
    "        valid_generator = data_generator.flow_from_directory(\n",
    "            directory=\"ham_labled/train\",\n",
    "            target_size=(IM_WIDTH, IM_HEIGHT),\n",
    "            shuffle=True,\n",
    "            color_mode=\"rgb\",\n",
    "            class_mode=\"categorical\",\n",
    "            batch_size=BATCH_SIZE,\n",
    "            subset=\"validation\"\n",
    "        )\n",
    "        # train\n",
    "        history = _model.fit_generator(\n",
    "            generator=train_generator,\n",
    "            steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "            validation_data=valid_generator,\n",
    "            validation_steps=valid_generator.samples//valid_generator.batch_size,\n",
    "            callbacks=h_callbacks,\n",
    "            epochs=EPOCHS\n",
    "        )\n",
    "        return _model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10556 images belonging to 7 classes.\n",
      "Found 2634 images belonging to 7 classes.\n",
      "Epoch 1/35\n",
      "164/164 [==============================] - 62s 380ms/step - loss: 2.2371 - acc: 0.3668 - categorical_accuracy: 0.3668 - top_2_accuracy: 0.5756 - top_3_accuracy: 0.7328 - val_loss: 7.9409 - val_acc: 0.1905 - val_categorical_accuracy: 0.1905 - val_top_2_accuracy: 0.4345 - val_top_3_accuracy: 0.5553\n",
      "Epoch 2/35\n",
      "164/164 [==============================] - 49s 299ms/step - loss: 1.5975 - acc: 0.4236 - categorical_accuracy: 0.4236 - top_2_accuracy: 0.6497 - top_3_accuracy: 0.8040 - val_loss: 2.7780 - val_acc: 0.2677 - val_categorical_accuracy: 0.2677 - val_top_2_accuracy: 0.4206 - val_top_3_accuracy: 0.5541\n",
      "Epoch 3/35\n",
      "164/164 [==============================] - 49s 300ms/step - loss: 1.3067 - acc: 0.4935 - categorical_accuracy: 0.4935 - top_2_accuracy: 0.7142 - top_3_accuracy: 0.8591 - val_loss: 1.4411 - val_acc: 0.4603 - val_categorical_accuracy: 0.4603 - val_top_2_accuracy: 0.6179 - val_top_3_accuracy: 0.7401\n",
      "Epoch 4/35\n",
      "164/164 [==============================] - 49s 300ms/step - loss: 1.1551 - acc: 0.5477 - categorical_accuracy: 0.5477 - top_2_accuracy: 0.7682 - top_3_accuracy: 0.8986 - val_loss: 4.8919 - val_acc: 0.1802 - val_categorical_accuracy: 0.1802 - val_top_2_accuracy: 0.3946 - val_top_3_accuracy: 0.4829\n",
      "Epoch 5/35\n",
      "164/164 [==============================] - 49s 300ms/step - loss: 1.0649 - acc: 0.5813 - categorical_accuracy: 0.5813 - top_2_accuracy: 0.8010 - top_3_accuracy: 0.9183 - val_loss: 2.0984 - val_acc: 0.4300 - val_categorical_accuracy: 0.4300 - val_top_2_accuracy: 0.6276 - val_top_3_accuracy: 0.7790\n",
      "Epoch 6/35\n",
      "Current Learning Rate: 0.0010000000474974513, Adjusted Learning Rate: 0.0001\n",
      "164/164 [==============================] - 49s 301ms/step - loss: 0.9095 - acc: 0.6367 - categorical_accuracy: 0.6367 - top_2_accuracy: 0.8453 - top_3_accuracy: 0.9442 - val_loss: 1.4085 - val_acc: 0.5160 - val_categorical_accuracy: 0.5160 - val_top_2_accuracy: 0.6693 - val_top_3_accuracy: 0.7486\n",
      "Epoch 7/35\n",
      "Current Learning Rate: 9.999999747378752e-05, Adjusted Learning Rate: 0.0001\n",
      "164/164 [==============================] - 49s 300ms/step - loss: 0.8532 - acc: 0.6742 - categorical_accuracy: 0.6742 - top_2_accuracy: 0.8662 - top_3_accuracy: 0.9579 - val_loss: 1.9866 - val_acc: 0.4358 - val_categorical_accuracy: 0.4358 - val_top_2_accuracy: 0.5965 - val_top_3_accuracy: 0.6829\n",
      "Epoch 8/35\n",
      "Current Learning Rate: 9.999999747378752e-05, Adjusted Learning Rate: 0.0001\n",
      "164/164 [==============================] - 49s 300ms/step - loss: 0.8046 - acc: 0.6862 - categorical_accuracy: 0.6862 - top_2_accuracy: 0.8791 - top_3_accuracy: 0.9631 - val_loss: 1.8365 - val_acc: 0.4673 - val_categorical_accuracy: 0.4673 - val_top_2_accuracy: 0.6000 - val_top_3_accuracy: 0.6922\n",
      "Epoch 9/35\n",
      "Current Learning Rate: 9.999999747378752e-05, Adjusted Learning Rate: 0.0001\n",
      "164/164 [==============================] - 49s 300ms/step - loss: 0.8537 - acc: 0.6755 - categorical_accuracy: 0.6755 - top_2_accuracy: 0.8655 - top_3_accuracy: 0.9527 - val_loss: 1.2990 - val_acc: 0.5031 - val_categorical_accuracy: 0.5031 - val_top_2_accuracy: 0.7202 - val_top_3_accuracy: 0.8401\n",
      "Epoch 10/35\n",
      "Current Learning Rate: 9.999999747378752e-05, Adjusted Learning Rate: 0.0001\n",
      "153/164 [==========================>...] - ETA: 2s - loss: 0.7530 - acc: 0.7138 - categorical_accuracy: 0.7138 - top_2_accuracy: 0.8869 - top_3_accuracy: 0.9670"
     ]
    }
   ],
   "source": [
    "model_ret, history_ret = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PLOTTING\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# plot the loss\n",
    "plt.plot(history_ret.history['loss'])\n",
    "plt.plot(history_ret.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREDICTION\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from imutils import paths\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1064 images belonging to 7 classes.\n",
      "{'akiec': 0, 'bcc': 1, 'bkl': 2, 'df': 3, 'mel': 4, 'nv': 5, 'vasc': 6}\n"
     ]
    }
   ],
   "source": [
    "# method-1\n",
    "def load_test_data(directory):\n",
    "    data = []\n",
    "    labels = []\n",
    "    image_paths = list(paths.list_images(directory))\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        # read\n",
    "        image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "        # keras format\n",
    "        image = img_to_array(image, data_format=\"channels_last\")\n",
    "        label = image_path.split(os.path.sep)[-2]\n",
    "        # append\n",
    "        data.append(image)\n",
    "        labels.append(label)\n",
    "    # conversions\n",
    "    data = np.array(data)\n",
    "    data = data.astype(\"float\")\n",
    "    return data, np.array(labels)\n",
    "\n",
    "# easy generator method\n",
    "data_generator = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = data_generator.flow_from_directory(\n",
    "    directory=\"ham_labled/test/\",\n",
    "    target_size=(IM_WIDTH, IM_HEIGHT),\n",
    "    shuffle=False,\n",
    "    color_mode=\"rgb\",\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=1\n",
    ")\n",
    "print(test_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.048485438908973\n",
      "val_cat_acc: 0.38533834586466165\n",
      "val_top_2_acc: 0.568609022556391\n",
      "val_top_3_acc: 0.6898496240601504\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_cat_acc, test_top_2_acc, test_top_3_acc = model_ret.evaluate_generator(test_generator, steps=test_generator.samples)\n",
    "\n",
    "print('val_loss:', test_loss)\n",
    "print('val_cat_acc:', test_cat_acc)\n",
    "print('val_top_2_acc:', test_top_2_acc)\n",
    "print('val_top_3_acc:', test_top_3_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1064/1064 [==============================] - 35s 33ms/step\n",
      "(1064, 7)\n"
     ]
    }
   ],
   "source": [
    "# PREDICT\n",
    "predictions = model_ret.predict_generator(test_generator, steps=test_generator.samples, verbose=1)\n",
    "print(predictions.shape)\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "y_test = test_generator.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      akiec       0.13      0.47      0.20        66\n",
      "        bcc       0.30      0.09      0.14       103\n",
      "        bkl       0.43      0.27      0.33       220\n",
      "         df       0.05      0.17      0.07        23\n",
      "        mel       0.56      0.02      0.04       223\n",
      "         nv       0.50      0.70      0.58       400\n",
      "       vasc       1.00      0.07      0.13        29\n",
      "\n",
      "avg / total       0.46      0.37      0.33      1064\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, y_pred, target_names=['akiec', 'bcc', 'bkl', 'df', 'mel','nv', 'vasc'])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
