{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>dx</th>\n",
       "      <th>dx_type</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>localization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10015</td>\n",
       "      <td>10015</td>\n",
       "      <td>10015</td>\n",
       "      <td>10015</td>\n",
       "      <td>9958.000000</td>\n",
       "      <td>10015</td>\n",
       "      <td>10015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>7470</td>\n",
       "      <td>10015</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>HAM_0003789</td>\n",
       "      <td>ISIC_0028440</td>\n",
       "      <td>nv</td>\n",
       "      <td>histo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6705</td>\n",
       "      <td>5340</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5406</td>\n",
       "      <td>2192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.863828</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.968614</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          lesion_id      image_id     dx dx_type          age    sex  \\\n",
       "count         10015         10015  10015   10015  9958.000000  10015   \n",
       "unique         7470         10015      7       4          NaN      3   \n",
       "top     HAM_0003789  ISIC_0028440     nv   histo          NaN   male   \n",
       "freq              6             1   6705    5340          NaN   5406   \n",
       "mean            NaN           NaN    NaN     NaN    51.863828    NaN   \n",
       "std             NaN           NaN    NaN     NaN    16.968614    NaN   \n",
       "min             NaN           NaN    NaN     NaN     0.000000    NaN   \n",
       "25%             NaN           NaN    NaN     NaN    40.000000    NaN   \n",
       "50%             NaN           NaN    NaN     NaN    50.000000    NaN   \n",
       "75%             NaN           NaN    NaN     NaN    65.000000    NaN   \n",
       "max             NaN           NaN    NaN     NaN    85.000000    NaN   \n",
       "\n",
       "       localization  \n",
       "count         10015  \n",
       "unique           15  \n",
       "top            back  \n",
       "freq           2192  \n",
       "mean            NaN  \n",
       "std             NaN  \n",
       "min             NaN  \n",
       "25%             NaN  \n",
       "50%             NaN  \n",
       "75%             NaN  \n",
       "max             NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = pd.read_csv(\"../HAM10000_metadata.csv\")\n",
    "metadata.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     lesion_id      image_id   dx dx_type   age   sex localization\n",
      "0  HAM_0000118  ISIC_0027419  bkl   histo  80.0  male        scalp\n",
      "1  HAM_0000118  ISIC_0025030  bkl   histo  80.0  male        scalp\n",
      "2  HAM_0002730  ISIC_0026769  bkl   histo  80.0  male        scalp\n",
      "3  HAM_0002730  ISIC_0025661  bkl   histo  80.0  male        scalp\n",
      "4  HAM_0001466  ISIC_0031633  bkl   histo  75.0  male          ear\n"
     ]
    }
   ],
   "source": [
    "print(metadata.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bkl', 'nv', 'df', 'mel', 'vasc', 'bcc', 'akiec']\n"
     ]
    }
   ],
   "source": [
    "labels = list(metadata.dx.unique())\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nv       6705\n",
      "mel      1113\n",
      "bkl      1099\n",
      "bcc       514\n",
      "akiec     327\n",
      "vasc      142\n",
      "df        115\n",
      "Name: dx, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x122ec2be0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax1 = plt.subplots(1, 1, figsize = (10, 5))\n",
    "counts = metadata[\"dx\"].value_counts()\n",
    "print(counts)\n",
    "counts.plot(kind='bar', ax=ax1)\n",
    "#metadata[\"dx\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse out all classes to individual dataframes\n",
    "nv = metadata[metadata[\"dx\"] == \"nv\"]\n",
    "nv.name = \"nv\"\n",
    "mel = metadata[metadata[\"dx\"] == \"mel\"]\n",
    "mel.name = \"mel\"\n",
    "bkl = metadata[metadata[\"dx\"] == \"bkl\"]\n",
    "bkl.name = \"bkl\"\n",
    "bcc = metadata[metadata[\"dx\"] == \"bcc\"]\n",
    "bcc.name = \"bcc\"\n",
    "akiec = metadata[metadata[\"dx\"] == \"akiec\"]\n",
    "akiec.name = \"akiec\"\n",
    "vasc = metadata[metadata[\"dx\"] == \"vasc\"]\n",
    "vasc.name = \"vasc\"\n",
    "df = metadata[metadata[\"dx\"] == \"df\"]\n",
    "df.name = \"df\"\n",
    "# list out\n",
    "classes = [nv, mel, bkl, bcc, akiec, vasc, df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess accordingly\n",
    "def preprocess(_image):\n",
    "    #_image = cv2.cvtColor(_image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    #_image = cv2.equalizeHist(_image) \n",
    "    #_image = cv2.GaussianBlur(_image, (3,3), 1)\n",
    "    _image = cv2.cvtColor(_image, cv2.COLOR_RGB2HSV)\n",
    "    H,S,V = cv2.split(_image)\n",
    "    _V = cv2.equalizeHist(V) \n",
    "    _image = cv2.merge([H, S, _V])\n",
    "    _image = cv2.cvtColor(_image, cv2.COLOR_HSV2RGB)\n",
    "    \n",
    "    return _image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# apply preprocesing and display both side by side for each class\\nnum_images = 2\\nfor _class in classes:\\n    # plotting\\n    fig, axes = plt.subplots(nrows=1, ncols=num_images, figsize=(15, 15), squeeze=False)\\n    fig.tight_layout()\\n    row = _class.sample(n=1)\\n    _id = row[\"image_id\"].to_string(index=False).strip()\\n    image = cv2.imread(\"../ham10000/{}.jpg\".format(_id))\\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\\n    image_preprocessed = preprocess(image)\\n    for l in range(1):\\n        for m, img in enumerate([image, image_preprocessed]):\\n            axes[l][m].imshow(img)\\n            axes[l][m].axis(\"off\")\\n            axes[l][m].set_title(\"Mean Pixel Value: {}\".format(np.mean(img)))\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# apply preprocesing and display both side by side for each class\n",
    "num_images = 2\n",
    "for _class in classes:\n",
    "    # plotting\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=num_images, figsize=(15, 15), squeeze=False)\n",
    "    fig.tight_layout()\n",
    "    row = _class.sample(n=1)\n",
    "    _id = row[\"image_id\"].to_string(index=False).strip()\n",
    "    image = cv2.imread(\"../ham10000/{}.jpg\".format(_id))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image_preprocessed = preprocess(image)\n",
    "    for l in range(1):\n",
    "        for m, img in enumerate([image, image_preprocessed]):\n",
    "            axes[l][m].imshow(img)\n",
    "            axes[l][m].axis(\"off\")\n",
    "            axes[l][m].set_title(\"Mean Pixel Value: {}\".format(np.mean(img)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate into Respective Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 5364, Test: 1341 images found for class nv\n",
      "Moving complete for class nv...\n",
      "Training: 890, Test: 223 images found for class mel\n",
      "Moving complete for class mel...\n",
      "Training: 879, Test: 220 images found for class bkl\n",
      "Moving complete for class bkl...\n",
      "Training: 411, Test: 103 images found for class bcc\n",
      "Moving complete for class bcc...\n",
      "Training: 261, Test: 66 images found for class akiec\n",
      "Moving complete for class akiec...\n",
      "Training: 113, Test: 29 images found for class vasc\n",
      "Moving complete for class vasc...\n",
      "Training: 92, Test: 23 images found for class df\n",
      "Moving complete for class df...\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# seperate images to each directory \n",
    "os.mkdir(\"ham_labled\")\n",
    "os.mkdir(\"ham_labled/train\")\n",
    "os.mkdir(\"ham_labled/test\")\n",
    "\n",
    "# each class\n",
    "for _class in classes:\n",
    "    _train, _test = train_test_split(_class, test_size = 0.2)\n",
    "    print(\"Training: {}, Test: {} images found for class {}\".format(len(_train), len(_test), _class.name))\n",
    "    # train \n",
    "    os.mkdir(\"ham_labled/train/{}\".format(_class.name))\n",
    "    for i, row in _train.iterrows():\n",
    "        image_id = row[\"image_id\"]\n",
    "        shutil.move(\n",
    "            src=\"../ham10000/{}.jpg\".format(image_id), \n",
    "            dst=\"ham_labled/train/{}/\".format(_class.name)\n",
    "        )\n",
    "    # test\n",
    "    os.mkdir(\"ham_labled/test/{}\".format(_class.name))\n",
    "    for i, row in _test.iterrows():\n",
    "        image_id = row[\"image_id\"]\n",
    "        shutil.move(\n",
    "            src=\"../ham10000/{}.jpg\".format(image_id), \n",
    "            dst=\"ham_labled/test/{}/\".format(_class.name)\n",
    "        )\n",
    "    print(\"Moving complete for class {}...\".format(_class.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Augumentation\n",
    "\n",
    "Before we can start passing above images through a CNN feature extractor, we will augument images to address class imbalance problem and set a threshold of 5000 images per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "_generator = ImageDataGenerator(\n",
    "    width_shift_range=0.001,\n",
    "    height_shift_range=0.001,\n",
    "    zoom_range=0.01,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    rotation_range=0.99\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(directory):\n",
    "    images = [cv2.cvtColor(cv2.imread(os.path.join(directory, file)), cv2.COLOR_BGR2RGB)\n",
    "              for file in os.listdir(directory)]\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augumenting images for class mel...\n",
      "Augumenting images for class bkl...\n",
      "Augumenting images for class bcc...\n",
      "Augumenting images for class akiec...\n",
      "Augumenting images for class vasc...\n",
      "Augumenting images for class df...\n"
     ]
    }
   ],
   "source": [
    "threshold = 5000\n",
    "for _class in classes:\n",
    "    _name = _class.name\n",
    "    if len(_class) >= threshold:\n",
    "        continue\n",
    "    else:\n",
    "        # what a pain ....PHEW!\n",
    "        if os.path.exists(\"ham_labled/train/{}/.ipynb_checkpoints/\".format(_name)):\n",
    "            shutil.rmtree(\"ham_labled/train/{}/.ipynb_checkpoints/\".format(_name))\n",
    "        print(\"Augumenting images for class {}...\".format(_name))\n",
    "        X = load_images(\"ham_labled/train/{}/\".format(_name))\n",
    "        _auguments = _generator.flow(\n",
    "            x=X,\n",
    "            y=None,\n",
    "            batch_size=32,\n",
    "            save_prefix=\"augumented\",\n",
    "            save_to_dir=\"ham_labled/train/{}/\".format(_name)\n",
    "        )\n",
    "        for _ in range(int(np.ceil((threshold - X.shape[0])/32))):\n",
    "            x = next(_auguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the newly generated Images and process them accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34092 total images found...\n"
     ]
    }
   ],
   "source": [
    "from imutils import paths\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "image_paths = list(paths.list_images(\"ham_labled/train\"))\n",
    "random.shuffle(image_paths)\n",
    "\n",
    "labels = [cls.split(os.path.sep)[-2] for cls in image_paths]\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)\n",
    "\n",
    "print(\"{} total images found...\".format(len(image_paths)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNet(object):\n",
    "\n",
    "    def __init__(self, height, width, channels, classes, parameter_scaling):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.channels = channels\n",
    "        self.output_classes = classes\n",
    "        self.scale = parameter_scaling\n",
    "\n",
    "    def model(self):\n",
    "        # initiate model\n",
    "        _model = Sequential()\n",
    "        input_shape = (self.height, self.width, self.channels)\n",
    "        axis = -1\n",
    "        # if using theano\n",
    "        if K.image_data_format() == \"channels_first\":\n",
    "            input_shape = (self.channels, self.height, self.width)\n",
    "            axis = 1\n",
    "\n",
    "        # conv_1\n",
    "        _model.add(Conv2D(\n",
    "            self.scale, (3, 3),\n",
    "            padding=\"same\",\n",
    "            input_shape=input_shape)\n",
    "        )\n",
    "        _model.add(Activation(\"relu\"))\n",
    "        _model.add(BatchNormalization(axis=axis))\n",
    "        # conv_2\n",
    "        _model.add(Conv2D(self.scale, (3, 3), padding=\"same\"))\n",
    "        _model.add(Activation(\"relu\"))\n",
    "        _model.add(BatchNormalization(axis=axis))\n",
    "        # pool_1\n",
    "        _model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        _model.add(Dropout(0.25))\n",
    "\n",
    "        # conv_3\n",
    "        _model.add(Conv2D(self.scale*2, (3, 3), padding=\"same\"))\n",
    "        _model.add(Activation(\"relu\"))\n",
    "        _model.add(BatchNormalization(axis=axis))\n",
    "        # conv_4\n",
    "        _model.add(Conv2D(self.scale*2, (3, 3), padding=\"same\"))\n",
    "        _model.add(Activation(\"relu\"))\n",
    "        _model.add(BatchNormalization(axis=axis))\n",
    "        # pool_2\n",
    "        _model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        _model.add(Dropout(0.25))\n",
    "        \n",
    "        # conv_5\n",
    "        _model.add(Conv2D(self.scale*3, (3, 3), padding=\"same\"))\n",
    "        _model.add(Activation(\"relu\"))\n",
    "        _model.add(BatchNormalization(axis=axis))\n",
    "        # conv_6\n",
    "        _model.add(Conv2D(self.scale*3, (3, 3), padding=\"same\"))\n",
    "        _model.add(Activation(\"relu\"))\n",
    "        _model.add(BatchNormalization(axis=axis))\n",
    "        # pool_3\n",
    "        _model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        _model.add(Dropout(0.25))\n",
    "\n",
    "        # Fully connected layers\n",
    "        _model.add(Flatten())\n",
    "        _model.add(Dense(512))\n",
    "        _model.add(Activation(\"relu\"))\n",
    "        _model.add(BatchNormalization())\n",
    "        _model.add(Dropout(0.5))\n",
    "        # classifier\n",
    "        _model.add(Dense(self.output_classes))\n",
    "        _model.add(Activation(\"softmax\"))\n",
    "\n",
    "        # return model\n",
    "        return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0806 08:02:26.862293 4544038336 deprecation_wrapper.py:119] From /Users/cn180450/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0806 08:02:26.864331 4544038336 deprecation_wrapper.py:119] From /Users/cn180450/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0806 08:02:26.872409 4544038336 deprecation_wrapper.py:119] From /Users/cn180450/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0806 08:02:26.901921 4544038336 deprecation_wrapper.py:119] From /Users/cn180450/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0806 08:02:26.902716 4544038336 deprecation_wrapper.py:119] From /Users/cn180450/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0806 08:02:26.991809 4544038336 deprecation_wrapper.py:119] From /Users/cn180450/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0806 08:02:27.103799 4544038336 deprecation_wrapper.py:119] From /Users/cn180450/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0806 08:02:27.108433 4544038336 deprecation.py:506] From /Users/cn180450/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0806 08:02:27.604783 4544038336 deprecation_wrapper.py:119] From /Users/cn180450/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27278 images belonging to 7 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0806 08:02:29.676442 4544038336 deprecation.py:323] From /Users/cn180450/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6814 images belonging to 7 classes.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`validation_steps=None` is only valid for a generator based on the `keras.utils.Sequence` class. Please specify `validation_steps` or use the `keras.utils.Sequence` class.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-44b32065c236>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-44b32065c236>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         )\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m     66\u001b[0m     if (val_gen and not isinstance(validation_data, Sequence) and\n\u001b[1;32m     67\u001b[0m             not validation_steps):\n\u001b[0;32m---> 68\u001b[0;31m         raise ValueError('`validation_steps=None` is only valid for a'\n\u001b[0m\u001b[1;32m     69\u001b[0m                          \u001b[0;34m' generator based on the `keras.utils.Sequence`'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                          \u001b[0;34m' class. Please specify `validation_steps` or use'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: `validation_steps=None` is only valid for a generator based on the `keras.utils.Sequence` class. Please specify `validation_steps` or use the `keras.utils.Sequence` class."
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "LR = 0.001\n",
    "EPOCHS = 50\n",
    "PARAMETER_SCALING = 64\n",
    "\n",
    "\n",
    "def train():\n",
    "        _model = CustomNet(\n",
    "            height=256,\n",
    "            width=256,\n",
    "            channels=3,\n",
    "            classes=7,\n",
    "            parameter_scaling=PARAMETER_SCALING\n",
    "        ).model()\n",
    "        _model.compile(\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            optimizer=Adam(lr=LR),\n",
    "            metrics=[\"accuracy\"]\n",
    "        )\n",
    "        \n",
    "        # generators made according to the post\n",
    "        # https://github.com/keras-team/keras/issues/5862\n",
    "        data_generator = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            validation_split=0.2\n",
    "        )\n",
    "        train_generator = data_generator.flow_from_directory(\n",
    "            directory=\"ham_labled/train\",\n",
    "            target_size=(256, 256),\n",
    "            color_mode=\"rgb\",\n",
    "            class_mode=\"categorical\",\n",
    "            batch_size=BATCH_SIZE,\n",
    "            subset=\"training\"\n",
    "        )\n",
    "        valid_generator = data_generator.flow_from_directory(\n",
    "            directory=\"ham_labled/train\",\n",
    "            target_size=(256, 256),\n",
    "            color_mode=\"rgb\",\n",
    "            class_mode=\"categorical\",\n",
    "            batch_size=BATCH_SIZE,\n",
    "            subset=\"validation\"\n",
    "        )\n",
    "        # train\n",
    "        history = _model.fit_generator(\n",
    "            generator=train_generator,\n",
    "            validation_data=valid_generator,\n",
    "            validation_steps=6814/BaA\n",
    "            steps_per_epoch=len(image_paths)//BATCH_SIZE,\n",
    "            epochs=EPOCHS\n",
    "        )\n",
    "        return _model, history\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction and CNN Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class HDF5IO has the utilities to read/write extracted features from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "class HDF5IO:\n",
    "\n",
    "    def __init__(self, dims, output_path, data_key=\"images\", buf_size=1000):\n",
    "        if os.path.exists(output_path):\n",
    "            raise ValueError(\n",
    "                \"The path already exists and cannot be overwritten\",\n",
    "                output_path\n",
    "            )\n",
    "        self.db = h5py.File(output_path, \"w\")\n",
    "        self.data = self.db.create_dataset(\n",
    "            data_key,\n",
    "            dims,\n",
    "            dtype=\"float\"\n",
    "        )\n",
    "        self.labels = self.db.create_dataset(\n",
    "            \"labels\",\n",
    "            (dims[0],),\n",
    "            dtype=\"int\"\n",
    "        )\n",
    "        self.buf_size = buf_size\n",
    "        self.buffer = {\"data\": [], \"labels\": []}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add(self, rows, labels):\n",
    "        \"\"\"\n",
    "        Adds the rows and labels to the buffer\n",
    "        :param rows:\n",
    "        :param labels:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.buffer[\"data\"].extend(rows)\n",
    "        self.buffer[\"labels\"].extend(labels)\n",
    "        # check to see if the buffer needs to be flushed to disk\n",
    "        if len(self.buffer[\"data\"]) >= self.buf_size:\n",
    "            self.flush()\n",
    "\n",
    "    def flush(self):\n",
    "        \"\"\"\n",
    "        Write the buffers to disk then reset the buffer\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        i = self.idx + len(self.buffer[\"data\"])\n",
    "        self.data[self.idx:i] = self.buffer[\"data\"]\n",
    "        self.labels[self.idx:i] = self.buffer[\"labels\"]\n",
    "        self.idx = i\n",
    "        self.buffer = {\"data\": [], \"labels\": []}\n",
    "\n",
    "    def store_class_labels(self, class_labels):\n",
    "        \"\"\"\n",
    "        create a dataset to store the actual class label names,\n",
    "        then store the class labels\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        dt = h5py.special_dtype(vlen=str)\n",
    "        label_set = self.db.create_dataset(\n",
    "            \"label_names\",\n",
    "            (len(class_labels),),\n",
    "            dtype=dt\n",
    "        )\n",
    "        label_set[:] = class_labels\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Closes the dataset\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if len(self.buffer[\"data\"]) > 0:\n",
    "            self.flush()\n",
    "        # close the dataset\n",
    "        self.db.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "from keras.applications import imagenet_utils\n",
    "\n",
    "# load the VGG16 model with imagenet weights\n",
    "model = VGG16(weights=\"imagenet\", include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = HDF5IO(\n",
    "    (len(image_paths), 512 * 7 * 7),\n",
    "    os.path.join(os.getcwd(), \"hmnist_features\"),\n",
    "    data_key=\"features\",\n",
    "    buf_size=1000\n",
    ")\n",
    "dataset.store_class_labels(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100% |#####################################| Time: 0:57:51\n"
     ]
    }
   ],
   "source": [
    "import progressbar\n",
    "\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "\n",
    "# initialize the progress bar\n",
    "widgets = [\n",
    "    \"Extracting Features: \", progressbar.Percentage(), \" \",\n",
    "    progressbar.Bar(), \" \", progressbar.ETA()\n",
    "]\n",
    "pbar = progressbar.ProgressBar(\n",
    "    maxval=len(image_paths),\n",
    "    widgets=widgets\n",
    ").start()\n",
    "\n",
    "bs = 32\n",
    "\n",
    "# loop over the images in patches\n",
    "for i in np.arange(0, len(image_paths), bs):\n",
    "    # extract the batch of images and labels, then initialize the\n",
    "    # list of actual images that will be passed through the network\n",
    "    # for feature extraction\n",
    "    batch_paths = image_paths[i:i + bs]\n",
    "    batch_labels = labels[i:i + bs]\n",
    "    batch_images = []\n",
    "\n",
    "    # loop over the images and labels in the current batch\n",
    "    for (j, image_path) in enumerate(batch_paths):\n",
    "        \n",
    "        image = load_img(image_path, target_size=(224, 224))\n",
    "        image = img_to_array(image)\n",
    "\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        image = imagenet_utils.preprocess_input(image)\n",
    "\n",
    "        # add the image to the batch\n",
    "        batch_images.append(image)\n",
    "\n",
    "    batch_images = np.vstack(batch_images)\n",
    "    features = model.predict(batch_images, batch_size=bs)\n",
    "\n",
    "    features = features.reshape((features.shape[0], 512 * 7 * 7))\n",
    "\n",
    "    # add the features and labels to our HDF5 dataset\n",
    "    dataset.add(features, batch_labels)\n",
    "    pbar.update(i)\n",
    "\n",
    "# close the dataset\n",
    "dataset.close()\n",
    "pbar.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading features from disk and applying a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the features from disk\n",
    "db = h5py.File(\"hmnist_features\", \"r\")\n",
    "\n",
    "# trian test split\n",
    "i = int(db[\"labels\"].shape[0] * 0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed: 15.8min\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed: 15.8min finished\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(\n",
    "    solver=\"lbfgs\", \n",
    "    multi_class=\"auto\",\n",
    "    n_jobs=-1,\n",
    "    C=10.0,\n",
    "    verbose=10.0\n",
    ")\n",
    "model.fit(db[\"features\"][:i], db[\"labels\"][:i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       akiec       0.00      0.00      0.00         0\n",
      "         bcc       0.87      0.92      0.89      2044\n",
      "         bkl       0.00      0.00      0.00         0\n",
      "          df       0.00      0.00      0.00         0\n",
      "         mel       0.00      0.00      0.00      5027\n",
      "          nv       0.00      0.00      0.00         0\n",
      "        vasc       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.27      0.27      0.27      7071\n",
      "   macro avg       0.12      0.13      0.13      7071\n",
      "weighted avg       0.25      0.27      0.26      7071\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cn180450/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cn180450/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "print(\"[INFO] evaluating...\")\n",
    "preds = model.predict(db[\"features\"][i:])\n",
    "print(\n",
    "    classification_report(\n",
    "        db[\"labels\"][i:], \n",
    "        preds,\n",
    "        target_names=db[\"label_names\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
