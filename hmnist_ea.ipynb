{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>dx</th>\n",
       "      <th>dx_type</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>localization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10015</td>\n",
       "      <td>10015</td>\n",
       "      <td>10015</td>\n",
       "      <td>10015</td>\n",
       "      <td>9958.000000</td>\n",
       "      <td>10015</td>\n",
       "      <td>10015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>7470</td>\n",
       "      <td>10015</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>HAM_0000835</td>\n",
       "      <td>ISIC_0028285</td>\n",
       "      <td>nv</td>\n",
       "      <td>histo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6705</td>\n",
       "      <td>5340</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5406</td>\n",
       "      <td>2192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.863828</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.968614</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          lesion_id      image_id     dx dx_type          age    sex  \\\n",
       "count         10015         10015  10015   10015  9958.000000  10015   \n",
       "unique         7470         10015      7       4          NaN      3   \n",
       "top     HAM_0000835  ISIC_0028285     nv   histo          NaN   male   \n",
       "freq              6             1   6705    5340          NaN   5406   \n",
       "mean            NaN           NaN    NaN     NaN    51.863828    NaN   \n",
       "std             NaN           NaN    NaN     NaN    16.968614    NaN   \n",
       "min             NaN           NaN    NaN     NaN     0.000000    NaN   \n",
       "25%             NaN           NaN    NaN     NaN    40.000000    NaN   \n",
       "50%             NaN           NaN    NaN     NaN    50.000000    NaN   \n",
       "75%             NaN           NaN    NaN     NaN    65.000000    NaN   \n",
       "max             NaN           NaN    NaN     NaN    85.000000    NaN   \n",
       "\n",
       "       localization  \n",
       "count         10015  \n",
       "unique           15  \n",
       "top            back  \n",
       "freq           2192  \n",
       "mean            NaN  \n",
       "std             NaN  \n",
       "min             NaN  \n",
       "25%             NaN  \n",
       "50%             NaN  \n",
       "75%             NaN  \n",
       "max             NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = pd.read_csv(\"../HAM10000_metadata.csv\")\n",
    "metadata.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     lesion_id      image_id   dx dx_type   age   sex localization\n",
      "0  HAM_0000118  ISIC_0027419  bkl   histo  80.0  male        scalp\n",
      "1  HAM_0000118  ISIC_0025030  bkl   histo  80.0  male        scalp\n",
      "2  HAM_0002730  ISIC_0026769  bkl   histo  80.0  male        scalp\n",
      "3  HAM_0002730  ISIC_0025661  bkl   histo  80.0  male        scalp\n",
      "4  HAM_0001466  ISIC_0031633  bkl   histo  75.0  male          ear\n"
     ]
    }
   ],
   "source": [
    "print(metadata.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bkl', 'nv', 'df', 'mel', 'vasc', 'bcc', 'akiec']\n"
     ]
    }
   ],
   "source": [
    "labels = list(metadata.dx.unique())\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nv       6705\n",
      "mel      1113\n",
      "bkl      1099\n",
      "bcc       514\n",
      "akiec     327\n",
      "vasc      142\n",
      "df        115\n",
      "Name: dx, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fed10fc8198>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax1 = plt.subplots(1, 1, figsize = (10, 5))\n",
    "counts = metadata[\"dx\"].value_counts()\n",
    "print(counts)\n",
    "counts.plot(kind='bar', ax=ax1)\n",
    "#metadata[\"dx\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse out all classes to individual dataframes\n",
    "nv = metadata[metadata[\"dx\"] == \"nv\"]\n",
    "nv.name = \"nv\"\n",
    "mel = metadata[metadata[\"dx\"] == \"mel\"]\n",
    "mel.name = \"mel\"\n",
    "bkl = metadata[metadata[\"dx\"] == \"bkl\"]\n",
    "bkl.name = \"bkl\"\n",
    "bcc = metadata[metadata[\"dx\"] == \"bcc\"]\n",
    "bcc.name = \"bcc\"\n",
    "akiec = metadata[metadata[\"dx\"] == \"akiec\"]\n",
    "akiec.name = \"akiec\"\n",
    "vasc = metadata[metadata[\"dx\"] == \"vasc\"]\n",
    "vasc.name = \"vasc\"\n",
    "df = metadata[metadata[\"dx\"] == \"df\"]\n",
    "df.name = \"df\"\n",
    "# list out\n",
    "classes = [nv, mel, bkl, bcc, akiec, vasc, df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess accordingly\n",
    "def preprocess(_image):\n",
    "    #_image = cv2.cvtColor(_image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    #_image = cv2.equalizeHist(_image) \n",
    "    #_image = cv2.GaussianBlur(_image, (3,3), 1)\n",
    "    _image = cv2.cvtColor(_image, cv2.COLOR_RGB2HSV)\n",
    "    H,S,V = cv2.split(_image)\n",
    "    _V = cv2.equalizeHist(V) \n",
    "    _image = cv2.merge([H, S, _V])\n",
    "    _image = cv2.cvtColor(_image, cv2.COLOR_HSV2RGB)\n",
    "    \n",
    "    return _image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate into Respective Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 5364, Test: 1341 images found for class nv\n",
      "Moving complete for class nv...\n",
      "Training: 890, Test: 223 images found for class mel\n",
      "Moving complete for class mel...\n",
      "Training: 879, Test: 220 images found for class bkl\n",
      "Moving complete for class bkl...\n",
      "Training: 411, Test: 103 images found for class bcc\n",
      "Moving complete for class bcc...\n",
      "Training: 261, Test: 66 images found for class akiec\n",
      "Moving complete for class akiec...\n",
      "Training: 113, Test: 29 images found for class vasc\n",
      "Moving complete for class vasc...\n",
      "Training: 92, Test: 23 images found for class df\n",
      "Moving complete for class df...\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# seperate images to each directory \n",
    "os.mkdir(\"ham_labled\")\n",
    "os.mkdir(\"ham_labled/train\")\n",
    "os.mkdir(\"ham_labled/test\")\n",
    "\n",
    "# each class\n",
    "for _class in classes:\n",
    "    _train, _test = train_test_split(_class, test_size = 0.2)\n",
    "    print(\"Training: {}, Test: {} images found for class {}\".format(len(_train), len(_test), _class.name))\n",
    "    # train \n",
    "    os.mkdir(\"ham_labled/train/{}\".format(_class.name))\n",
    "    for i, row in _train.iterrows():\n",
    "        image_id = row[\"image_id\"]\n",
    "        img = cv2.imread(\"../ham10000/{}.jpg\".format(image_id))\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        cv2.imwrite(\"../ham10000/{}.jpg\".format(image_id), img)\n",
    "        shutil.move(\n",
    "            src=\"../ham10000/{}.jpg\".format(image_id), \n",
    "            dst=\"ham_labled/train/{}/\".format(_class.name)\n",
    "        )\n",
    "    # test\n",
    "    os.mkdir(\"ham_labled/test/{}\".format(_class.name))\n",
    "    for i, row in _test.iterrows():\n",
    "        image_id = row[\"image_id\"]\n",
    "        img = cv2.imread(\"../ham10000/{}.jpg\".format(image_id))\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        cv2.imwrite(\"../ham10000/{}.jpg\".format(image_id), img)\n",
    "        shutil.move(\n",
    "            src=\"../ham10000/{}.jpg\".format(image_id), \n",
    "            dst=\"ham_labled/test/{}/\".format(_class.name)\n",
    "        )\n",
    "    print(\"Moving complete for class {}...\".format(_class.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Augumentation\n",
    "\n",
    "Before we can start passing above images through a CNN feature extractor, we will augument images to address class imbalance problem and set a threshold of 5000 images per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "_generator = ImageDataGenerator(\n",
    "    width_shift_range=0.001,\n",
    "    height_shift_range=0.001,\n",
    "    zoom_range=0.01,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    rotation_range=0.99\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "def load_images(directory):\n",
    "    images = [cv2.cvtColor(cv2.imread(os.path.join(directory, file)), cv2.COLOR_BGR2RGB)\n",
    "              for file in os.listdir(directory)]\n",
    "    images = [img_to_array(image, data_format=\"channels_last\") for image in images]\n",
    "    images = np.array(images)\n",
    "    images = images.astype(\"float\")\n",
    "    return images\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augumenting images for class mel...\n",
      "Augumenting images for class bkl...\n",
      "Augumenting images for class bcc...\n",
      "Augumenting images for class akiec...\n",
      "Augumenting images for class vasc...\n",
      "Augumenting images for class df...\n"
     ]
    }
   ],
   "source": [
    "threshold = 3000\n",
    "batch = 32\n",
    "for _class in classes:\n",
    "    _name = _class.name\n",
    "    if len(_class) >= threshold:\n",
    "        continue\n",
    "    else:\n",
    "        # what a pain ....PHEW!\n",
    "        if os.path.exists(\"ham_labled/train/{}/.ipynb_checkpoints/\".format(_name)):\n",
    "            shutil.rmtree(\"ham_labled/train/{}/.ipynb_checkpoints/\".format(_name))\n",
    "        print(\"Augumenting images for class {}...\".format(_name))\n",
    "        X = load_images(\"ham_labled/train/{}/\".format(_name))\n",
    "        _auguments = _generator.flow(\n",
    "            x=X,\n",
    "            y=None,\n",
    "            batch_size=batch,\n",
    "            save_format=\"jpg\",\n",
    "            save_prefix=\"augumented\",\n",
    "            save_to_dir=\"ham_labled/train/{}/\".format(_name)\n",
    "        )\n",
    "        for _ in range(int(np.ceil((threshold - X.shape[0])/batch))):\n",
    "            x = next(_auguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the newly generated Images and process them accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imutils\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/0c/659c2bdae8e8ca5ef810b9da02db28feaa29ea448ff36b65a1664ff28142/imutils-0.5.2.tar.gz\n",
      "Building wheels for collected packages: imutils\n",
      "  Running setup.py bdist_wheel for imutils ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/b2/40/59/139d450e68847ef2f27d876d527b13389dac23df0f66526b5d\n",
      "Successfully built imutils\n",
      "\u001b[31mmenpo 0.8.1 has requirement matplotlib<2.0,>=1.4, but you'll have matplotlib 3.0.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mmenpo 0.8.1 has requirement pillow<5.0,>=3.0, but you'll have pillow 5.4.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mmenpo 0.8.1 has requirement scipy<1.0,>=0.16, but you'll have scipy 1.2.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: imutils\n",
      "Successfully installed imutils-0.5.2\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.2.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "22658 total images found...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from imutils import paths\n",
    "except ModuleNotFoundError:\n",
    "    !pip install imutils\n",
    "    from imutils import paths\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "image_paths = list(paths.list_images(\"ham_labled/train\"))\n",
    "random.shuffle(image_paths)\n",
    "\n",
    "labels = [cls.split(os.path.sep)[-2] for cls in image_paths]\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)\n",
    "\n",
    "print(\"{} total images found...\".format(len(image_paths)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import callbacks\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNet(object):\n",
    "\n",
    "    def __init__(self, height, width, channels, classes, parameter_scaling):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.channels = channels\n",
    "        self.output_classes = classes\n",
    "        self.scale = parameter_scaling\n",
    "\n",
    "    def model(self):\n",
    "        # initiate model\n",
    "        _model = Sequential()\n",
    "        input_shape = (self.height, self.width, self.channels)\n",
    "        axis = -1\n",
    "        # if using theano\n",
    "        if K.image_data_format() == \"channels_first\":\n",
    "            input_shape = (self.channels, self.height, self.width)\n",
    "            axis = 1\n",
    "\n",
    "        # conv_1\n",
    "        _model.add(Conv2D(\n",
    "            self.scale, (3, 3),\n",
    "            padding=\"same\",\n",
    "            input_shape=input_shape)\n",
    "        )\n",
    "        _model.add(Activation(\"relu\"))\n",
    "        _model.add(BatchNormalization(axis=axis))\n",
    "        # conv_2\n",
    "        _model.add(Conv2D(self.scale, (3, 3), padding=\"same\"))\n",
    "        _model.add(Activation(\"relu\"))\n",
    "        _model.add(BatchNormalization(axis=axis))\n",
    "        # pool_1\n",
    "        _model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        _model.add(Dropout(0.25))\n",
    "\n",
    "        # conv_3\n",
    "        _model.add(Conv2D(self.scale*2, (3, 3), padding=\"same\"))\n",
    "        _model.add(Activation(\"relu\"))\n",
    "        _model.add(BatchNormalization(axis=axis))\n",
    "        # conv_4\n",
    "        _model.add(Conv2D(self.scale*2, (3, 3), padding=\"same\"))\n",
    "        _model.add(Activation(\"relu\"))\n",
    "        _model.add(BatchNormalization(axis=axis))\n",
    "        # pool_2\n",
    "        _model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        _model.add(Dropout(0.25))\n",
    "        \"\"\"\n",
    "        # conv_5\n",
    "        _model.add(Conv2D(self.scale*3, (3, 3), padding=\"same\"))\n",
    "        _model.add(Activation(\"relu\"))\n",
    "        _model.add(BatchNormalization(axis=axis))\n",
    "        # conv_6\n",
    "        _model.add(Conv2D(self.scale*3, (3, 3), padding=\"same\"))\n",
    "        _model.add(Activation(\"relu\"))\n",
    "        _model.add(BatchNormalization(axis=axis))\n",
    "        # conv_7\n",
    "        _model.add(Conv2D(self.scale*3, (3, 3), padding=\"same\"))\n",
    "        _model.add(Activation(\"relu\"))\n",
    "        _model.add(BatchNormalization(axis=axis))\n",
    "        # pool_3\n",
    "        _model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        _model.add(Dropout(0.25))\n",
    "        \"\"\"\n",
    "\n",
    "        # Fully connected layers\n",
    "        _model.add(Flatten())\n",
    "        _model.add(Dense(512))\n",
    "        _model.add(Activation(\"relu\"))\n",
    "        _model.add(BatchNormalization())\n",
    "        _model.add(Dropout(0.5))\n",
    "        # classifier\n",
    "        _model.add(Dense(self.output_classes))\n",
    "        _model.add(Activation(\"softmax\"))\n",
    "\n",
    "        # return model\n",
    "        return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "manager = multiprocessing.Manager()\n",
    "queue = manager.Queue(maxsize=10)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.001\n",
    "EPOCHS = 50\n",
    "PARAMETER_SCALING = 32\n",
    "IM_WIDTH = 224\n",
    "IM_HEIGHT = 224\n",
    "\n",
    "def train():\n",
    "    \n",
    "        h_callbacks = [\n",
    "            callbacks.TensorBoard(\n",
    "                log_dir=\"tensorboard\",\n",
    "                write_graph=True,\n",
    "                write_images=False\n",
    "            )\n",
    "        ]\n",
    "        _model = CustomNet(\n",
    "            height=IM_HEIGHT,\n",
    "            width=IM_WIDTH,\n",
    "            channels=3,\n",
    "            classes=7,\n",
    "            parameter_scaling=PARAMETER_SCALING\n",
    "        ).model()\n",
    "        _model.compile(\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            optimizer=Adam(lr=LR),\n",
    "            metrics=[\"accuracy\"]\n",
    "        )\n",
    "        \n",
    "        # generators made according to the post\n",
    "        # https://github.com/keras-team/keras/issues/5862\n",
    "        data_generator = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            validation_split=0.2\n",
    "        )\n",
    "        train_generator = data_generator.flow_from_directory(\n",
    "            directory=\"ham_labled/train\",\n",
    "            target_size=(IM_WIDTH, IM_HEIGHT),\n",
    "            shuffle=True,\n",
    "            color_mode=\"rgb\",\n",
    "            class_mode=\"categorical\",\n",
    "            batch_size=BATCH_SIZE,\n",
    "            subset=\"training\"\n",
    "        )\n",
    "        valid_generator = data_generator.flow_from_directory(\n",
    "            directory=\"ham_labled/train\",\n",
    "            target_size=(IM_WIDTH, IM_HEIGHT),\n",
    "            shuffle=True,\n",
    "            color_mode=\"rgb\",\n",
    "            class_mode=\"categorical\",\n",
    "            batch_size=BATCH_SIZE,\n",
    "            subset=\"validation\"\n",
    "        )\n",
    "        # train\n",
    "        history = _model.fit_generator(\n",
    "            generator=train_generator,\n",
    "            steps_per_epoch=train_generator.n//train_generator.batch_size,\n",
    "            validation_data=valid_generator,\n",
    "            validation_steps=valid_generator.n//valid_generator.batch_size,\n",
    "            callbacks=h_callbacks,\n",
    "            epochs=EPOCHS\n",
    "        )\n",
    "        return _model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18130 images belonging to 7 classes.\n",
      "Found 4528 images belonging to 7 classes.\n",
      "Epoch 1/50\n",
      "283/283 [==============================] - 214s 757ms/step - loss: 1.3767 - acc: 0.5464 - val_loss: 10.9307 - val_acc: 0.2594\n",
      "Epoch 2/50\n",
      "283/283 [==============================] - 212s 749ms/step - loss: 0.7611 - acc: 0.7248 - val_loss: 1.9807 - val_acc: 0.3761\n",
      "Epoch 3/50\n",
      "283/283 [==============================] - 212s 751ms/step - loss: 0.4881 - acc: 0.8244 - val_loss: 2.8059 - val_acc: 0.4890\n",
      "Epoch 4/50\n",
      "283/283 [==============================] - 212s 751ms/step - loss: 0.3425 - acc: 0.8817 - val_loss: 2.0362 - val_acc: 0.4433\n",
      "Epoch 5/50\n",
      "283/283 [==============================] - 212s 750ms/step - loss: 0.2416 - acc: 0.9152 - val_loss: 2.1703 - val_acc: 0.4879\n",
      "Epoch 6/50\n",
      "283/283 [==============================] - 212s 750ms/step - loss: 0.1810 - acc: 0.9395 - val_loss: 2.9050 - val_acc: 0.4189\n",
      "Epoch 7/50\n",
      "283/283 [==============================] - 212s 750ms/step - loss: 0.1321 - acc: 0.9541 - val_loss: 3.0881 - val_acc: 0.4250\n",
      "Epoch 8/50\n",
      "283/283 [==============================] - 212s 750ms/step - loss: 0.2208 - acc: 0.9243 - val_loss: 4.5668 - val_acc: 0.3575\n",
      "Epoch 9/50\n",
      "283/283 [==============================] - 212s 750ms/step - loss: 0.1354 - acc: 0.9539 - val_loss: 3.1494 - val_acc: 0.4447\n",
      "Epoch 10/50\n",
      "283/283 [==============================] - 212s 750ms/step - loss: 0.0687 - acc: 0.9775 - val_loss: 2.3736 - val_acc: 0.4711\n",
      "Epoch 11/50\n",
      "283/283 [==============================] - 212s 749ms/step - loss: 0.0538 - acc: 0.9827 - val_loss: 2.5256 - val_acc: 0.5347\n",
      "Epoch 12/50\n",
      "283/283 [==============================] - 212s 750ms/step - loss: 0.0383 - acc: 0.9875 - val_loss: 3.2675 - val_acc: 0.4689\n",
      "Epoch 13/50\n",
      "283/283 [==============================] - 212s 750ms/step - loss: 0.0328 - acc: 0.9900 - val_loss: 2.4222 - val_acc: 0.5605\n",
      "Epoch 14/50\n",
      "283/283 [==============================] - 212s 749ms/step - loss: 0.0387 - acc: 0.9881 - val_loss: 2.6131 - val_acc: 0.5388\n",
      "Epoch 15/50\n",
      "283/283 [==============================] - 212s 750ms/step - loss: 0.0257 - acc: 0.9923 - val_loss: 4.0743 - val_acc: 0.4404\n",
      "Epoch 16/50\n",
      "283/283 [==============================] - 212s 750ms/step - loss: 0.0407 - acc: 0.9856 - val_loss: 3.4162 - val_acc: 0.4733\n",
      "Epoch 17/50\n",
      "283/283 [==============================] - 212s 750ms/step - loss: 0.0321 - acc: 0.9895 - val_loss: 3.6422 - val_acc: 0.4619\n",
      "Epoch 18/50\n",
      "283/283 [==============================] - 212s 751ms/step - loss: 0.0389 - acc: 0.9867 - val_loss: 4.5170 - val_acc: 0.4534\n",
      "Epoch 19/50\n",
      "106/283 [==========>...................] - ETA: 2:02 - loss: 0.0364 - acc: 0.9874"
     ]
    }
   ],
   "source": [
    "model_ret, history_ret = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 th image of 22658\n",
      "1000 th image of 22658\n",
      "2000 th image of 22658\n",
      "3000 th image of 22658\n",
      "4000 th image of 22658\n",
      "5000 th image of 22658\n",
      "6000 th image of 22658\n",
      "7000 th image of 22658\n",
      "8000 th image of 22658\n",
      "9000 th image of 22658\n",
      "10000 th image of 22658\n",
      "11000 th image of 22658\n",
      "12000 th image of 22658\n",
      "13000 th image of 22658\n",
      "14000 th image of 22658\n",
      "15000 th image of 22658\n",
      "16000 th image of 22658\n",
      "17000 th image of 22658\n",
      "18000 th image of 22658\n",
      "19000 th image of 22658\n",
      "20000 th image of 22658\n",
      "21000 th image of 22658\n",
      "22000 th image of 22658\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "for i,im in enumerate(image_paths):\n",
    "    if i%1000 == 0:\n",
    "        print(\"{} th image of {}\".format(i, len(image_paths)))\n",
    "    try:\n",
    "        img = Image.open(im)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(i)\n",
    "        print(im)\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mat is not a numpy array, neither a scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b21d46e644f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ImageWindow\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ham_labled/train/bkl/augumented_668_3668.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: mat is not a numpy array, neither a scalar"
     ]
    }
   ],
   "source": [
    "cv2.imshow(\"ImageWindow\", \"ham_labled/train/bkl/augumented_668_3668.png\")\n",
    "cv2.waitKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction and CNN Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class HDF5IO has the utilities to read/write extracted features from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "class HDF5IO:\n",
    "\n",
    "    def __init__(self, dims, output_path, data_key=\"images\", buf_size=1000):\n",
    "        if os.path.exists(output_path):\n",
    "            raise ValueError(\n",
    "                \"The path already exists and cannot be overwritten\",\n",
    "                output_path\n",
    "            )\n",
    "        self.db = h5py.File(output_path, \"w\")\n",
    "        self.data = self.db.create_dataset(\n",
    "            data_key,\n",
    "            dims,\n",
    "            dtype=\"float\"\n",
    "        )\n",
    "        self.labels = self.db.create_dataset(\n",
    "            \"labels\",\n",
    "            (dims[0],),\n",
    "            dtype=\"int\"\n",
    "        )\n",
    "        self.buf_size = buf_size\n",
    "        self.buffer = {\"data\": [], \"labels\": []}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add(self, rows, labels):\n",
    "        \"\"\"\n",
    "        Adds the rows and labels to the buffer\n",
    "        :param rows:\n",
    "        :param labels:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.buffer[\"data\"].extend(rows)\n",
    "        self.buffer[\"labels\"].extend(labels)\n",
    "        # check to see if the buffer needs to be flushed to disk\n",
    "        if len(self.buffer[\"data\"]) >= self.buf_size:\n",
    "            self.flush()\n",
    "\n",
    "    def flush(self):\n",
    "        \"\"\"\n",
    "        Write the buffers to disk then reset the buffer\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        i = self.idx + len(self.buffer[\"data\"])\n",
    "        self.data[self.idx:i] = self.buffer[\"data\"]\n",
    "        self.labels[self.idx:i] = self.buffer[\"labels\"]\n",
    "        self.idx = i\n",
    "        self.buffer = {\"data\": [], \"labels\": []}\n",
    "\n",
    "    def store_class_labels(self, class_labels):\n",
    "        \"\"\"\n",
    "        create a dataset to store the actual class label names,\n",
    "        then store the class labels\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        dt = h5py.special_dtype(vlen=str)\n",
    "        label_set = self.db.create_dataset(\n",
    "            \"label_names\",\n",
    "            (len(class_labels),),\n",
    "            dtype=dt\n",
    "        )\n",
    "        label_set[:] = class_labels\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Closes the dataset\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if len(self.buffer[\"data\"]) > 0:\n",
    "            self.flush()\n",
    "        # close the dataset\n",
    "        self.db.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "from keras.applications import imagenet_utils\n",
    "\n",
    "# load the VGG16 model with imagenet weights\n",
    "model = VGG16(weights=\"imagenet\", include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = HDF5IO(\n",
    "    (len(image_paths), 512 * 7 * 7),\n",
    "    os.path.join(os.getcwd(), \"hmnist_features\"),\n",
    "    data_key=\"features\",\n",
    "    buf_size=1000\n",
    ")\n",
    "dataset.store_class_labels(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100% |#####################################| Time: 0:57:51\n"
     ]
    }
   ],
   "source": [
    "import progressbar\n",
    "\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "\n",
    "# initialize the progress bar\n",
    "widgets = [\n",
    "    \"Extracting Features: \", progressbar.Percentage(), \" \",\n",
    "    progressbar.Bar(), \" \", progressbar.ETA()\n",
    "]\n",
    "pbar = progressbar.ProgressBar(\n",
    "    maxval=len(image_paths),\n",
    "    widgets=widgets\n",
    ").start()\n",
    "\n",
    "bs = 32\n",
    "\n",
    "# loop over the images in patches\n",
    "for i in np.arange(0, len(image_paths), bs):\n",
    "    # extract the batch of images and labels, then initialize the\n",
    "    # list of actual images that will be passed through the network\n",
    "    # for feature extraction\n",
    "    batch_paths = image_paths[i:i + bs]\n",
    "    batch_labels = labels[i:i + bs]\n",
    "    batch_images = []\n",
    "\n",
    "    # loop over the images and labels in the current batch\n",
    "    for (j, image_path) in enumerate(batch_paths):\n",
    "        \n",
    "        image = load_img(image_path, target_size=(224, 224))\n",
    "        image = img_to_array(image)\n",
    "\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        image = imagenet_utils.preprocess_input(image)\n",
    "\n",
    "        # add the image to the batch\n",
    "        batch_images.append(image)\n",
    "\n",
    "    batch_images = np.vstack(batch_images)\n",
    "    features = model.predict(batch_images, batch_size=bs)\n",
    "\n",
    "    features = features.reshape((features.shape[0], 512 * 7 * 7))\n",
    "\n",
    "    # add the features and labels to our HDF5 dataset\n",
    "    dataset.add(features, batch_labels)\n",
    "    pbar.update(i)\n",
    "\n",
    "# close the dataset\n",
    "dataset.close()\n",
    "pbar.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading features from disk and applying a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the features from disk\n",
    "db = h5py.File(\"hmnist_features\", \"r\")\n",
    "\n",
    "# trian test split\n",
    "i = int(db[\"labels\"].shape[0] * 0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed: 15.8min\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed: 15.8min finished\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(\n",
    "    solver=\"lbfgs\", \n",
    "    multi_class=\"auto\",\n",
    "    n_jobs=-1,\n",
    "    C=10.0,\n",
    "    verbose=10.0\n",
    ")\n",
    "model.fit(db[\"features\"][:i], db[\"labels\"][:i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       akiec       0.00      0.00      0.00         0\n",
      "         bcc       0.87      0.92      0.89      2044\n",
      "         bkl       0.00      0.00      0.00         0\n",
      "          df       0.00      0.00      0.00         0\n",
      "         mel       0.00      0.00      0.00      5027\n",
      "          nv       0.00      0.00      0.00         0\n",
      "        vasc       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.27      0.27      0.27      7071\n",
      "   macro avg       0.12      0.13      0.13      7071\n",
      "weighted avg       0.25      0.27      0.26      7071\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cn180450/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/cn180450/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "print(\"[INFO] evaluating...\")\n",
    "preds = model.predict(db[\"features\"][i:])\n",
    "print(\n",
    "    classification_report(\n",
    "        db[\"labels\"][i:], \n",
    "        preds,\n",
    "        target_names=db[\"label_names\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
